"""Tool API routes using LangGraph tool graph."""

import json
import logging
import asyncio
import contextlib
from fastapi import APIRouter, Query
from fastapi.responses import StreamingResponse
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage

from graphs.tool_graph import graph as tool_graph

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

router = APIRouter(prefix="/tool", tags=["tool"])


@router.get("/")
async def tool_endpoint(
    message: str = Query(..., description="User input message"),
    thread_id: str = Query("default", description="Thread ID for conversation memory")
):
    """
    Blocking tool endpoint.
    
    Query parameter:
    - message: user input text
    
    Returns:
    - JSON response with AI response and tool usage info
    """
    try:
        # Create initial state with user message
        initial_state = {
            "messages": [HumanMessage(content=message)]
        }
        
        # Create config with thread_id for conversation memory
        config = {"configurable": {"thread_id": thread_id}}
        
        logger.info(f"Invoking tool graph for message: '{message}' with thread_id: {thread_id}")
        # Invoke the tool graph and get final result
        result = await tool_graph.ainvoke(initial_state, config)
        logger.info(f"Graph invocation finished. Final state: {result}")
        
        # Extract the assistant's response
        assistant_message = result["messages"][-1]
        
        # Check if tools were used by looking at message history
        tool_calls_used = []
        for msg in result["messages"]:
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                for tool_call in msg.tool_calls:
                    tool_calls_used.append({
                        "name": tool_call.get("name", "unknown"),
                        "args": tool_call.get("args", {})
                    })
        
        return {
            "success": True,
            "response": assistant_message.content,
            "message_count": len(result["messages"]),
            "tools_used": tool_calls_used,
            "used_tools": len(tool_calls_used) > 0
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "response": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ã€‚"
        }


@router.get("/stream")
async def tool_stream_endpoint(
    message: str = Query(..., description="User input message"),
    thread_id: str = Query("default", description="Thread ID for conversation memory")
):
    """
    Streaming tool endpoint using official LangGraph streaming with tool usage display.
    
    Query parameter:
    - message: user input text
    
    Returns:
    - Server-Sent Events (SSE) stream of response tokens and tool info
    """
    
    async def generate_tool_stream():
        try:
            logger.info(f"[tool_routes][start] å¼€å§‹å·¥å…·æµå¼è¯·æ±‚: {message[:50]}...")
            
            # Send start signal
            start_data = {
                'type': 'start', 
                'content': '', 
                'metadata': {'node': 'system', 'step': 0}
            }
            yield f"data: {json.dumps(start_data)}\n\n"
            
            # Create initial state with user message
            initial_state = {
                "messages": [HumanMessage(content=message)]
            }
            logger.info(f"[tool_routes][init] åˆ›å»ºåˆå§‹çŠ¶æ€ï¼Œæ¶ˆæ¯æ•°é‡: {len(initial_state['messages'])}")
            
            # Use LangGraph's official streaming with multiple modes
            logger.info("[tool_routes][stream] ä½¿ç”¨ LangGraph å®˜æ–¹å¤šæ¨¡å¼æµå¼è¾“å‡º: stream_mode=['messages','updates']")
            
            accumulated_content = ""
            chunk_count = 0
            current_node = None
            tool_decision_sent = False  # æ ‡è®°æ˜¯å¦å·²å‘é€AIå†³ç­–äº‹ä»¶
            
            # Create config with thread_id for conversation memory
            config = {"configurable": {"thread_id": thread_id}}
            
            # Stream using LangGraph's official streaming API with multiple modes
            async for stream_mode, chunk in tool_graph.astream(
                initial_state,
                config,
                stream_mode=["messages", "updates"]
            ):
                # stream_mode="messages" å’Œ stream_mode="updates" å‘å›æ¥çš„ æ•°æ®ç²’åº¦å®Œå…¨ä¸åŒï¼Œå› æ­¤ä¸¤ç§æ—¥å¿—çœ‹èµ·æ¥ä¼šå¾ˆä¸ä¸€æ ·â€”â€”è¿™æ­£æ˜¯å®˜æ–¹è®¾è®¡çš„ç»“æœã€‚
                #
                # 1. messages â€”â€” LLM å­—ç¬¦çº§ / token çº§æµ
                # è¿›å…¥ ä»»ä½• LLM èŠ‚ç‚¹ï¼ˆå¦‚
                # chatbot
                # ï¼‰æ—¶ï¼ŒLangGraph ä¼šæŠŠ LLM äº§ç”Ÿçš„ token æŒ‰ æœ€å°é¢—ç²’åº¦ ä¾æ¬¡æ¨é€ã€‚
                # ä½ çš„å¾ªç¯é‡Œæ”¶åˆ°çš„æ˜¯
                # (message_chunk, metadata)
                # ï¼Œå…¶ä¸­ message_chunk.content å¾€å¾€åªæ˜¯ä¸€å°æ®µï¼ˆå‡ ä¸ª tokenï¼‰ã€‚
                # è¿™äº› token å› ä¸ºè¢«ä½  é€ä¸ª yield ç»™å‰ç«¯ï¼Œæ‰€ä»¥å‰ç«¯æ‰èƒ½å®æ—¶æ‰“å­—æœºå¼æ¸²æŸ“ã€‚
                # åœ¨ messages æ¨¡å¼ä¸‹ï¼Œä½ å·²ç»è¿‡æ»¤æ‰æ¥è‡ª tools èŠ‚ç‚¹çš„ chunkï¼Œå› æ­¤ç°åœ¨åªä¼šçœ‹åˆ°çœŸæ­£çš„ LLM token æµã€‚
                # 2. updates â€”â€” èŠ‚ç‚¹çº§ / æ­¥éª¤çº§çŠ¶æ€å¿«ç…§
                # æ¯å½“å›¾ä¸­ä»»æ„èŠ‚ç‚¹æ‰§è¡Œå®Œæˆï¼ŒLangGraph å°±ä¼šå‘ä¸€æ¬¡ updatesã€‚
                # æ•°æ®ç»“æ„æ˜¯ï¼š
                # python
                # {
                #    "<node_name>": <node_output>
                # }
                # è¿™é‡Œçš„ <node_output> å¾€å¾€å·²ç»æ˜¯ å®Œæ•´çš„å¯¹è±¡ â€”â€”
                #
                # chatbot
                #  èŠ‚ç‚¹ï¼š{"messages": [AIMessage(...)]}
                # tools èŠ‚ç‚¹ï¼š{"messages": [ToolMessage(...)]}
                # å› ä¸ºå®ƒæœ¬æ¥å°±æ˜¯â€œèŠ‚ç‚¹å®Œæˆåï¼ŒæŠŠç»“æœæ•´ä½“æ‰“åŒ…ç»™ä½ â€ï¼Œæ‰€ä»¥çœ‹èµ·æ¥å†…å®¹ä¸€æ¬¡å°±å¾ˆâ€œå®Œæ•´â€ã€‚
                # æ¢å¥è¯è¯´ï¼š
                #
                # æ¨¡å¼	ä½•æ—¶è§¦å‘	å…¸å‹å†…å®¹	ç”¨é€”
                # messages	LLM æ­£åœ¨ç”Ÿæˆå›å¤æ—¶	å¾ˆçŸ­çš„ token ç‰‡æ®µ	æ‰“å­—æœºæµ
                # updates	å›¾ä¸­æŸèŠ‚ç‚¹åˆšç»“æŸæ—¶	æ•´ä¸ª AIMessage / ToolMessage åˆ—è¡¨	è¿›åº¦ & ç»“æœé€šçŸ¥
                if stream_mode == "messages":
                    # Handle LLM token streaming
                    message_chunk, metadata = chunk
                    node_from_metadata = metadata.get('langgraph_node', 'unknown')
                    # ç»Ÿä¸€çš„ messages åˆ†æ”¯æ—¥å¿—å‰ç¼€
                    prefix = f"[tool_routes][messages][node={node_from_metadata}]"
                    logger.info(
                        f"{prefix} chunk åˆ°è¾¾: content='{getattr(message_chunk, 'content', '')}', has_tool_calls={hasattr(message_chunk, 'tool_calls')}")

                    # å¦‚æœæ¥è‡ª ToolNodeï¼ˆtools èŠ‚ç‚¹ï¼‰çš„æ¶ˆæ¯ï¼Œåˆ™è·³è¿‡ï¼Œé¿å…å°†å·¥å…·è¿”å›å€¼æ˜¾ç¤ºä¸º AI å›å¤
                    if node_from_metadata == "tools":
                        # è¿™æ˜¯å·¥å…·èŠ‚ç‚¹çš„ token æµï¼ˆé€šå¸¸ä»£è¡¨ ToolMessage ç›¸å…³è¾“å‡ºç‰‡æ®µï¼Œå·¥å…·tool nodeæ‰§è¡Œåçš„è¿”å›å€¼ï¼‰ï¼Œè·³è¿‡æ˜¾ç¤ºï¼Œä»…è®°å½•æ—¥å¿—ã€‚
                        logger.info(f"{prefix}[ToolMessageChunk][skip] å·¥å…·èŠ‚ç‚¹ chunk å·²è·³è¿‡ï¼Œcontent='{getattr(message_chunk, 'content', '')}'")
                        continue
                    if hasattr(message_chunk, 'content') and message_chunk.content:
                        # å¯¹äº chatbot èŠ‚ç‚¹ï¼Œè¿™äº› chunk å±äº AIMessage çš„ token çº§è¾“å‡º
                        logger.info(f"{prefix}[AIMessageChunk] token='{message_chunk.content}'")
                        chunk_count += 1
                        accumulated_content += message_chunk.content
                        
                        # Send the streaming token
                        chunk_data = {
                            'type': 'content',
                            'content': message_chunk.content,
                            'metadata': {
                                'node': metadata.get('langgraph_node', 'unknown'),
                                'chunk_number': chunk_count,
                                'accumulated_length': len(accumulated_content),
                                'langgraph_metadata': metadata
                            }
                        }
                        yield f"data: {json.dumps(chunk_data)}\n\n"
                        
                elif stream_mode == "updates":
                    # Handle node state updates
                    for node_name, node_output in chunk.items():
                        current_node = node_name
                        prefix = f"[tool_routes][updates][node={node_name}]"
                        logger.info(f"{prefix} èŠ‚ç‚¹çŠ¶æ€æ›´æ–°åˆ°è¾¾")
                        
                        if node_name == "tools":
                            # Tool node execution
                            logger.info(f"{prefix}[ToolNode] ğŸ”§ å·¥å…·èŠ‚ç‚¹æ­£åœ¨æ‰§è¡Œ...")
                            if "messages" in node_output and node_output["messages"]:
                                # ToolMessageï¼ˆå®Œæ•´å¯¹è±¡ï¼‰
                                tool_message = node_output["messages"][-1]
                                if isinstance(tool_message, ToolMessage):
                                    logger.info(f"{prefix}[ToolMessage] âœ… å·¥å…·æ‰§è¡Œå®Œæˆï¼Œcontent_len={len(tool_message.content) if hasattr(tool_message, 'content') else 0}")
                                else:
                                    logger.info(f"{prefix}[ToolMessage?] âœ… å·¥å…·æ‰§è¡Œå®Œæˆï¼Œç±»å‹={type(tool_message)}")
                                if hasattr(tool_message, 'content'):
                                    tool_data = {
                                        'type': 'tool_result',
                                        'content': f"ğŸ”§ å·¥å…·æ‰§è¡Œå®Œæˆ",
                                        'result': tool_message.content,
                                        'metadata': {'node': node_name}
                                    }
                                    yield f"data: {json.dumps(tool_data)}\n\n"
                        
                        elif node_name == "chatbot":
                            # Chatbot node - check for tool calls
                            if "messages" in node_output and node_output["messages"]:
                                # AIMessageï¼ˆå®Œæ•´å¯¹è±¡ï¼‰
                                ai_message = node_output["messages"][-1]

                                # è¯¦ç»†è®°å½• AIMessage ä¿¡æ¯
                                if isinstance(ai_message, AIMessage):
                                    logger.info(f"{prefix}[AIMessage] ğŸ¤– ChatbotèŠ‚ç‚¹è¿”å›AIMessageï¼Œcontent_len={len(ai_message.content) if hasattr(ai_message, 'content') else 0}")
                                else:
                                    logger.info(f"{prefix}[AIMessage?] ğŸ¤– ChatbotèŠ‚ç‚¹è¿”å›æ¶ˆæ¯ï¼Œä½†ç±»å‹={type(ai_message)}")
                                logger.info(f"{prefix}[AIMessage] has_tool_calls={hasattr(ai_message, 'tool_calls') and bool(ai_message.tool_calls)}")
                                if hasattr(ai_message, 'tool_calls') and ai_message.tool_calls:
                                    logger.info(f"{prefix}[AIMessage] tool_calls={[tc.get('name', 'unknown') for tc in ai_message.tool_calls]}")
                                    for i, tc in enumerate(ai_message.tool_calls):
                                        logger.info(f"{prefix}[AIMessage]   [{i}] name={tc.get('name', 'unknown')} args={tc.get('args', {})}")
                                
                                # Check if the message has tool calls and not already sent
                                if hasattr(ai_message, 'tool_calls') and ai_message.tool_calls and not tool_decision_sent:
                                    logger.info(f"{prefix}[AIMessage] ğŸ” AIå†³å®šè°ƒç”¨å·¥å…·: {[tc.get('name', 'unknown') for tc in ai_message.tool_calls]}")
                                    
                                    # æ ‡è®°å·²å‘é€ï¼Œé¿å…é‡å¤
                                    tool_decision_sent = True
                                    
                                    # å‘é€AIå†³å®šè°ƒç”¨å·¥å…·çš„äº‹ä»¶
                                    decision_data = {
                                        'type': 'ai_decision',
                                        'content': 'ğŸ¤– AIå†³å®šè°ƒç”¨å·¥å…·',
                                        'metadata': {'node': node_name}
                                    }
                                    logger.info(f"{prefix}[AIMessage] ğŸ“¤ å‘é€AIå†³ç­–äº‹ä»¶: {decision_data}")
                                    yield f"data: {json.dumps(decision_data)}\n\n"
                                    
                                    # ç¨å¾®å»¶è¿Ÿä»¥ç¡®ä¿äº‹ä»¶é¡ºåº
                                    import asyncio
                                    await asyncio.sleep(0.01)
                                    
                                    # å‘é€æ¯ä¸ªå·¥å…·è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯
                                    for tool_call in ai_message.tool_calls:
                                        tool_call_data = {
                                            'type': 'tool_call',
                                            'content': f"ğŸ” å‡†å¤‡è°ƒç”¨å·¥å…·: {tool_call.get('name', 'unknown')}",
                                            'tool_name': tool_call.get('name', 'unknown'),
                                            'tool_args': tool_call.get('args', {}),
                                            'metadata': {'node': node_name}
                                        }
                                        logger.info(f"{prefix}[AIMessage] ğŸ“¤ å‘é€å·¥å…·è°ƒç”¨äº‹ä»¶: {tool_call_data}")
                                        yield f"data: {json.dumps(tool_call_data)}\n\n"
                                        await asyncio.sleep(0.01)  # å°å»¶è¿Ÿç¡®ä¿äº‹ä»¶é¡ºåº

            
            # Send completion signal
            end_data = {
                'type': 'end',
                'content': '',
                'metadata': {
                    'total_chunks': chunk_count,
                    'total_length': len(accumulated_content),
                    'final_node': current_node
                }
            }
            yield f"data: {json.dumps(end_data)}\n\n"
            logger.info(f"[tool_routes][end] å·¥å…·æµå¼è¾“å‡ºå®Œæˆï¼Œæ€»å…± {chunk_count} ä¸ª tokenï¼Œæ€»é•¿åº¦: {len(accumulated_content)} å­—ç¬¦")
            
        except Exception as e:
            # Send error signal
            error_msg = f"å¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
            error_data = {
                'type': 'error', 
                'content': error_msg, 
                'metadata': {'node': 'system', 'error': True}
            }
            yield f"data: {json.dumps(error_data)}\n\n"
            logger.error(f"[tool_routes][error] å·¥å…·æµå¼è¾“å‡ºå‡ºé”™: {str(e)}", exc_info=True)
    
    return StreamingResponse(
        generate_tool_stream(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream"
        }
    )
