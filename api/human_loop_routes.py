import asyncio
import logging
from typing import Dict, Any, Optional
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import json
import uuid

from langgraph.types import Command
from langgraph.errors import GraphInterrupt

from graphs.human_loop_graph import graph

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter(prefix="/human-loop", tags=["human-loop"])

class ChatRequest(BaseModel):
    message: str
    thread_id: Optional[str] = "default"

class HumanResponse(BaseModel):
    thread_id: str
    response: str


    query: Optional[str] = None

@router.post("/chat")
async def chat_with_human_loop(request: ChatRequest):
    """
    å¯åŠ¨å¯¹è¯ï¼Œæ”¯æŒäººå·¥å¹²é¢„
    """
    try:
        config = {"configurable": {"thread_id": request.thread_id}}
        initial_state = {"messages": [{"role": "user", "content": request.message}]}
        
        logger.info(f"å¼€å§‹å¤„ç†å¯¹è¯ï¼Œthread_id: {request.thread_id}")
        
        try:
            # ä½¿ç”¨invokeè¿›è¡Œé˜»å¡è°ƒç”¨ï¼Œä¸chat_routes.pyä¿æŒä¸€è‡´
            logger.info(f"å¼€å§‹æ‰§è¡Œå›¾ï¼Œåˆå§‹çŠ¶æ€: {initial_state}")
            
            result = await graph.ainvoke(initial_state, config)
            
            # æ£€æŸ¥æœ€ç»ˆæ¶ˆæ¯
            if result and "messages" in result and result["messages"]:
                final_message = result["messages"][-1]
                logger.info(f"æœ€ç»ˆæ¶ˆæ¯: {final_message}")
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ï¼ˆå¯èƒ½éœ€è¦äººå·¥å¹²é¢„ï¼‰
                if hasattr(final_message, 'tool_calls') and final_message.tool_calls:
                    # æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†human_assistanceå·¥å…·
                    for tool_call in final_message.tool_calls:
                        if tool_call.get('name') == 'human_assistance':
                            query = tool_call.get('args', {}).get('query', 'éœ€è¦äººå·¥ååŠ©')
                            logger.info(f"æ£€æµ‹åˆ°human_assistanceå·¥å…·è°ƒç”¨ï¼ŒæŸ¥è¯¢: {query}")
                            
                            return {
                                "intervention_required": True,
                                "thread_id": request.thread_id,
                                "query": query,
                                "status": "intervention_required"
                            }
                
                # æ­£å¸¸çš„AIå›å¤
                if hasattr(final_message, 'content') and final_message.content:
                    response_text = final_message.content
                elif hasattr(final_message, 'text'):
                    response_text = final_message.text() if callable(final_message.text) else final_message.text
                else:
                    response_text = str(final_message)
                    
                logger.info(f"è¿”å›å“åº”: {response_text}")
                return {
                    "response": response_text,
                    "thread_id": request.thread_id,
                    "status": "completed"
                }
            else:
                logger.warning("æœªæ”¶åˆ°ä»»ä½•æ¶ˆæ¯")
                return {
                    "response": "æŠ±æ­‰ï¼ŒAIæš‚æ—¶æ— æ³•å›å¤ï¼Œè¯·ç¨åå†è¯•ã€‚",
                    "thread_id": request.thread_id,
                    "status": "completed"
                }
            
        except GraphInterrupt as e:
            # æ•è·äººå·¥å¹²é¢„ä¸­æ–­
            logger.info(f"æ•è·åˆ°äººå·¥å¹²é¢„è¯·æ±‚: {e}")
            
            # è·å–ä¸­æ–­ä¿¡æ¯
            interrupt_data = e.interrupts[0] if e.interrupts else {}
            query = interrupt_data.get("query", "éœ€è¦äººå·¥ååŠ©")
            
            return {
                "intervention_required": True,
                "thread_id": request.thread_id,
                "query": query,
                "status": "intervention_required"
            }
            
    except Exception as e:
        logger.error(f"å¯¹è¯å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¯¹è¯å¤„ç†å¤±è´¥: {str(e)}")

@router.post("/respond")
async def provide_human_response(request: HumanResponse):
    """
    æä¾›äººå·¥å›å¤ï¼Œæ¢å¤å›¾æ‰§è¡Œ
    """
    try:
        config = {"configurable": {"thread_id": request.thread_id}}
        
        # åˆ›å»ºæ¢å¤å‘½ä»¤
        human_command = Command(resume={"data": request.response})
        
        logger.info(f"æ¢å¤å›¾æ‰§è¡Œï¼Œthread_id: {request.thread_id}")
        
        # æ¢å¤æ‰§è¡Œå¹¶è·å–ç»“æœ
        final_message = None
        logger.info(f"æ¢å¤æ‰§è¡Œå›¾ï¼Œä½¿ç”¨å‘½ä»¤: {human_command}")
        
        async for event in graph.astream(human_command, config, stream_mode="values"):
            logger.info(f"æ¢å¤æ‰§è¡Œæ”¶åˆ°äº‹ä»¶: {event}")
            if "messages" in event and event["messages"]:
                final_message = event["messages"][-1]
                logger.info(f"æ¢å¤æ‰§è¡Œæœ€æ–°æ¶ˆæ¯: {final_message}")
        

        
        # æ£€æŸ¥æœ€ç»ˆæ¶ˆæ¯
        if final_message:
            if hasattr(final_message, 'content') and final_message.content:
                response_text = final_message.content
            elif hasattr(final_message, 'text'):
                response_text = final_message.text() if callable(final_message.text) else final_message.text
            else:
                response_text = str(final_message)
                
            logger.info(f"äººå·¥å›å¤å¤„ç†å®Œæˆï¼Œè¿”å›: {response_text}")
            return {
                "response": response_text,
                "thread_id": request.thread_id,
                "status": "completed"
            }
        else:
            logger.warning("äººå·¥å›å¤å¤„ç†åæœªæ”¶åˆ°æ¶ˆæ¯")
            return {
                "response": "äººå·¥ååŠ©å¤„ç†å®Œæˆ",
                "thread_id": request.thread_id,
                "status": "completed"
            }
        
    except Exception as e:
        logger.error(f"äººå·¥å›å¤å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"äººå·¥å›å¤å¤„ç†å¤±è´¥: {str(e)}")





@router.get("/chat/stream")
async def stream_chat_with_human_loop(message: str, thread_id: str = "default"):
    """
    æµå¼å¯¹è¯ï¼Œæ”¯æŒäººå·¥å¹²é¢„
    """
    async def generate():
        logger.info(f"å¼€å§‹å·¥å…·æµå¼è¯·æ±‚: {message[:50]}...")
        try:
            # start äº‹ä»¶å¯¹é½ tool_routes.py
            start_data = {
                'type': 'start',
                'content': '',
                'metadata': {'node': 'system', 'step': 0}
            }
            yield f"data: {json.dumps(start_data)}\n\n"

            # initial_state ä¸ config
            config = {"configurable": {"thread_id": thread_id}}
            initial_state = {"messages": [{"role": "user", "content": message}]}

            # ç´¯ç§¯å˜é‡å¯¹é½ tool_routes.py
            accumulated_content = ""
            chunk_count = 0
            current_node = None
            tool_decision_sent = False

            logger.info(f"[stream_chat] è°ƒç”¨ graph.astreamï¼Œinitial_state={initial_state}")
            try:
                # åŒæ—¶è·å– messages ä¸ updates
                async for stream_mode, chunk in graph.astream(
                    initial_state,
                    config,
                    stream_mode=["messages", "updates"]
                ):
                    if stream_mode == "messages":
                        # Handle LLM token streamingï¼ˆå¯¹é½ tool_routes.pyï¼‰
                        message_chunk, metadata = chunk
                        node_from_metadata = metadata.get('langgraph_node', 'unknown')
                        # å¦‚æœæ¥è‡ª tools èŠ‚ç‚¹çš„æ¶ˆæ¯ï¼Œåˆ™è·³è¿‡
                        if node_from_metadata == "tools":
                            logger.info(f"è·å–äº†LLMæ¶ˆæ¯ - è°ƒç”¨å·¥å…·è¿”å›: {getattr(message_chunk, 'content', '')}")
                            continue
                        if hasattr(message_chunk, 'content') and message_chunk.content:
                            logger.info(f"è·å–äº†LLMæ¶ˆæ¯: {message_chunk.content}")
                            chunk_count += 1
                            accumulated_content += message_chunk.content
                            # å‘é€ content äº‹ä»¶ï¼ˆç»“æ„å¯¹é½ï¼‰
                            chunk_data = {
                                'type': 'content',
                                'content': message_chunk.content,
                                'metadata': {
                                    'node': metadata.get('langgraph_node', 'unknown'),
                                    'chunk_number': chunk_count,
                                    'accumulated_length': len(accumulated_content),
                                    'langgraph_metadata': metadata
                                }
                            }
                            yield f"data: {json.dumps(chunk_data)}\n\n"

                    elif stream_mode == "updates":
                        # å¤„ç†èŠ‚ç‚¹æ›´æ–°ï¼ˆå¯¹é½ tool_routes.pyï¼‰
                        for node_name, node_output in chunk.items():
                            current_node = node_name
                            logger.info(f"èŠ‚ç‚¹æ›´æ–° - {node_name}: {str(node_output)}")

                            # äººå·¥å¹²é¢„ï¼šLangGraph åœ¨ updates æµä¸­ä»¥ä¸­æ–­èŠ‚ç‚¹å½¢å¼ä¸ŠæŠ¥ï¼Œä¸ä¸€å®šæŠ›å¼‚å¸¸
                            if node_name in {"__interrupt__", "interrupt", "graph:interrupt"}:
                                # è§£æä¸­æ–­è´Ÿè½½ï¼Œå…¼å®¹ tuple/list/obj ä¸‰ç§å½¢å¼
                                interrupt_payload = None
                                try:
                                    candidate = None
                                    if isinstance(node_output, (list, tuple)) and node_output:
                                        candidate = node_output[0]
                                    else:
                                        candidate = node_output
                                    if hasattr(candidate, "value"):
                                        interrupt_payload = candidate.value
                                    elif isinstance(candidate, dict) and "value" in candidate:
                                        interrupt_payload = candidate.get("value")
                                except Exception:
                                    interrupt_payload = None

                                query = None
                                if isinstance(interrupt_payload, dict):
                                    query = interrupt_payload.get("query")
                                if not query:
                                    query = "éœ€è¦äººå·¥ååŠ©"

                                logger.info(f"[stream_chat] æ£€æµ‹åˆ°ä¸­æ–­èŠ‚ç‚¹ï¼Œè§¦å‘äººå·¥å¹²é¢„ï¼Œquery={query}")
                                intervention_event = {
                                    'type': 'intervention_required',
                                    'query': query,
                                    'thread_id': thread_id,
                                    'metadata': {'node': node_name}
                                }
                                yield f"data: {json.dumps(intervention_event, ensure_ascii=False)}\n\n"

                                # å‘é€ç»“æŸäº‹ä»¶å¹¶é€€å‡ºæµ
                                end_data = {
                                    'type': 'end',
                                    'content': '',
                                    'metadata': {
                                        'total_chunks': chunk_count,
                                        'total_length': len(accumulated_content),
                                        'final_node': current_node
                                    }
                                }
                                yield f"data: {json.dumps(end_data)}\n\n"
                                return

                            if node_name == "tools":
                                logger.info("ğŸ”§ å·¥å…·èŠ‚ç‚¹æ­£åœ¨æ‰§è¡Œ...")
                                if "messages" in node_output and node_output["messages"]:
                                    tool_message = node_output["messages"][-1]
                                    if hasattr(tool_message, 'content'):
                                        logger.info(f"âœ… å·¥å…·æ‰§è¡Œå®Œæˆ: {tool_message.content}")
                                        tool_data = {
                                            'type': 'tool_result',
                                            'content': 'ğŸ”§ å·¥å…·æ‰§è¡Œå®Œæˆ',
                                            'result': tool_message.content,
                                            'metadata': {'node': node_name}
                                        }
                                        yield f"data: {json.dumps(tool_data)}\n\n"

                            elif node_name == "chatbot":
                                if "messages" in node_output and node_output["messages"]:
                                    ai_message = node_output["messages"][-1]
                                    if hasattr(ai_message, 'tool_calls') and ai_message.tool_calls and not tool_decision_sent:
                                        logger.info(f"ğŸ” AIå†³å®šè°ƒç”¨å·¥å…·: {[tc.get('name', 'unknown') for tc in ai_message.tool_calls]}")
                                        tool_decision_sent = True
                                        decision_data = {
                                            'type': 'ai_decision',
                                            'content': 'ğŸ¤– AIå†³å®šè°ƒç”¨å·¥å…·',
                                            'metadata': {'node': node_name}
                                        }
                                        logger.info(f"ğŸ“¤ å‘é€AIå†³ç­–äº‹ä»¶: {decision_data}")
                                        yield f"data: {json.dumps(decision_data)}\n\n"
                                        import asyncio
                                        await asyncio.sleep(0.01)

                                        for tool_call in ai_message.tool_calls:
                                            tool_call_data = {
                                                'type': 'tool_call',
                                                'content': f"ğŸ” å‡†å¤‡è°ƒç”¨å·¥å…·: {tool_call.get('name', 'unknown')}",
                                                'tool_name': tool_call.get('name', 'unknown'),
                                                'tool_args': tool_call.get('args', {}),
                                                'metadata': {'node': node_name}
                                            }
                                            logger.info(f"ğŸ“¤ å‘é€å·¥å…·è°ƒç”¨äº‹ä»¶: {tool_call_data}")
                                            yield f"data: {json.dumps(tool_call_data)}\n\n"
                                            await asyncio.sleep(0.01)

                # end äº‹ä»¶å¯¹é½ tool_routes.py
                end_data = {
                    'type': 'end',
                    'content': '',
                    'metadata': {
                        'total_chunks': chunk_count,
                        'total_length': len(accumulated_content),
                        'final_node': current_node
                    }
                }
                yield f"data: {json.dumps(end_data)}\n\n"
            except GraphInterrupt as e:
                # HITL: æ•è·äººå·¥å¹²é¢„ä¸­æ–­å¹¶é€šçŸ¥å‰ç«¯
                interrupt_data = e.interrupts[0] if e.interrupts else {}
                query = interrupt_data.get("query", "éœ€è¦äººå·¥ååŠ©")
                logger.info(f"[stream_chat] è§¦å‘äººå·¥å¹²é¢„ï¼Œquery={query}")
                yield f"data: {json.dumps({'type': 'intervention_required', 'query': query, 'thread_id': thread_id}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logger.error(f"[stream_chat] æµå¼å¯¹è¯å¤±è´¥: {str(e)}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)}, ensure_ascii=False)}\n\n"
            logger.debug("[stream_chat] å·²å‘é€ error äº‹ä»¶")
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        },
    )


@router.get("/respond/stream")
async def stream_respond_with_human_input(response: str, thread_id: str):
    """
    äººå·¥å›å¤åï¼Œæµå¼æ¢å¤å›¾æ‰§è¡Œå¹¶è¿”å›æœ€ç»ˆAIå›å¤ï¼ˆSSEï¼‰ã€‚
    - ä¸ /human-loop/chat/stream çš„äº‹ä»¶ç»“æ„ä¿æŒä¸€è‡´ï¼šstart/content/ai_decision/tool_call/tool_result/intervention_required/end
    - é¿å…å·¥å…·èŠ‚ç‚¹æ¶ˆæ¯è¢«æ‹†åˆ†ä¸ºå¤šæ®µï¼šè·³è¿‡ messages æµä¸­çš„ tools èŠ‚ç‚¹åˆ†ç‰‡
    """
    async def generate():
        logger.info(f"[respond_stream] å¼€å§‹äººå·¥å›å¤æµå¼æ¢å¤: thread_id={thread_id}, response={response[:50]}...")
        try:
            # start äº‹ä»¶
            start_data = {'type': 'start', 'content': '', 'metadata': {'node': 'system', 'step': 0}}
            yield f"data: {json.dumps(start_data)}\n\n"

            config = {"configurable": {"thread_id": thread_id}}
            human_command = Command(resume={"data": response})

            accumulated_content = ""
            chunk_count = 0
            current_node = None
            tool_decision_sent = False

            # æ¢å¤å¹¶ä»¥å¤šæ¨¡å¼æµå¼è¿”å›
            async for stream_mode, chunk in graph.astream(
                human_command, config, stream_mode=["messages", "updates"]
            ):
                if stream_mode == "messages":
                    message_chunk, metadata = chunk
                    node_from_metadata = metadata.get('langgraph_node', 'unknown')
                    if node_from_metadata == "tools":
                        logger.info(f"[respond_stream] è·³è¿‡toolsæ¶ˆæ¯åˆ†ç‰‡: {getattr(message_chunk, 'content', '')}")
                        continue
                    if hasattr(message_chunk, 'content') and message_chunk.content:
                        logger.info(f"[respond_stream] æ”¶åˆ°LLMå†…å®¹åˆ†ç‰‡: {message_chunk.content}")
                        chunk_count += 1
                        accumulated_content += message_chunk.content
                        yield f"data: {json.dumps({'type':'content','content':message_chunk.content,'metadata':{'node':node_from_metadata,'chunk_number':chunk_count,'accumulated_length':len(accumulated_content),'langgraph_metadata':metadata}})}\n\n"

                elif stream_mode == "updates":
                    for node_name, node_output in chunk.items():
                        current_node = node_name
                        logger.info(f"[respond_stream] èŠ‚ç‚¹æ›´æ–° - {node_name}: {str(node_output)}")

                        # å¤„ç†ä¸­æ–­èŠ‚ç‚¹ï¼ˆå¯èƒ½å‡ºç°å†æ¬¡äººå·¥å¹²é¢„ï¼‰
                        if node_name in {"__interrupt__", "interrupt", "graph:interrupt"}:
                            interrupt_payload = None
                            try:
                                candidate = node_output[0] if isinstance(node_output, (list, tuple)) and node_output else node_output
                                if hasattr(candidate, "value"):
                                    interrupt_payload = candidate.value
                                elif isinstance(candidate, dict) and "value" in candidate:
                                    interrupt_payload = candidate.get("value")
                            except Exception:
                                interrupt_payload = None
                            query = interrupt_payload.get("query") if isinstance(interrupt_payload, dict) else None
                            if not query:
                                query = "éœ€è¦äººå·¥ååŠ©"
                            yield f"data: {json.dumps({'type':'intervention_required','query':query,'thread_id':thread_id,'metadata':{'node':node_name}}, ensure_ascii=False)}\n\n"
                            end_data = {'type':'end','content':'','metadata':{'total_chunks':chunk_count,'total_length':len(accumulated_content),'final_node':current_node}}
                            yield f"data: {json.dumps(end_data)}\n\n"
                            return

                        if node_name == "tools":
                            if "messages" in node_output and node_output["messages"]:
                                tool_message = node_output["messages"][-1]
                                if hasattr(tool_message, 'content'):
                                    tool_data = {'type':'tool_result','content':'ğŸ”§ å·¥å…·æ‰§è¡Œå®Œæˆ','result':tool_message.content,'metadata':{'node':node_name}}
                                    yield f"data: {json.dumps(tool_data)}\n\n"

                        elif node_name == "chatbot":
                            if "messages" in node_output and node_output["messages"]:
                                ai_message = node_output["messages"][-1]
                                if hasattr(ai_message, 'tool_calls') and ai_message.tool_calls and not tool_decision_sent:
                                    tool_decision_sent = True
                                    decision_data = {'type':'ai_decision','content':'ğŸ¤– AIå†³å®šè°ƒç”¨å·¥å…·','metadata':{'node':node_name}}
                                    yield f"data: {json.dumps(decision_data)}\n\n"
                                    import asyncio
                                    await asyncio.sleep(0.01)
                                    for tool_call in ai_message.tool_calls:
                                        tool_call_data = {
                                            'type': 'tool_call',
                                            'content': f"ğŸ” å‡†å¤‡è°ƒç”¨å·¥å…·: {tool_call.get('name','unknown')}",
                                            'tool_name': tool_call.get('name','unknown'),
                                            'tool_args': tool_call.get('args', {}),
                                            'metadata': {'node': node_name}
                                        }
                                        yield f"data: {json.dumps(tool_call_data)}\n\n"
                                        await asyncio.sleep(0.01)

            end_data = {'type':'end','content':'','metadata':{'total_chunks':chunk_count,'total_length':len(accumulated_content),'final_node':current_node}}
            yield f"data: {json.dumps(end_data)}\n\n"

        except Exception as e:
            logger.error(f"[respond_stream] æµå¼æ¢å¤å¤±è´¥: {str(e)}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)}, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        },
    )
