import logging
from typing import Optional, Dict, Any
import json
import asyncio

from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from langgraph.types import Command
from langgraph.errors import GraphInterrupt

from graphs.customize_state_graph import graph


# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter(prefix="/customize-state", tags=["customize-state"])


class ChatRequest(BaseModel):
    message: str
    thread_id: Optional[str] = "default"


class HumanReview(BaseModel):
    thread_id: str
    # è‹¥æä¾› correct ä¸”ä»¥ y/Y å¼€å¤´ï¼Œåˆ™è§†ä¸ºç¡®è®¤æ— è¯¯ï¼›å¦åˆ™å¯æä¾› name/birthday ä½œä¸ºä¿®æ­£
    correct: Optional[str] = None
    name: Optional[str] = None
    birthday: Optional[str] = None


@router.post("/chat")
async def chat_customize_state(request: ChatRequest) -> Dict[str, Any]:
    """
    å¯åŠ¨å¯¹è¯ï¼Œè¯·æ±‚ customize_state å›¾ã€‚è‹¥éœ€è¦äººå·¥å®¡é˜…ï¼Œä¼šè§¦å‘ GraphInterruptã€‚
    è¿”å›ï¼š
    - æ­£å¸¸å®Œæˆï¼š{"response": str, "thread_id": str, "status": "completed"}
    - éœ€è¦äººå·¥å®¡é˜…ï¼š{"intervention_required": True, "thread_id": str, "question": str, "name": str, "birthday": str, "status": "intervention_required"}
    """
    try:
        config = {"configurable": {"thread_id": request.thread_id}}
        initial_state = {"messages": [{"role": "user", "content": request.message}]}

        logger.info(f"[customize_state.chat] thread_id={request.thread_id} initial_state={initial_state}")

        try:
            result = await graph.ainvoke(initial_state, config)
            # æ­£å¸¸å®Œæˆï¼Œè¿”å›æœ€åä¸€æ¡æ¶ˆæ¯å†…å®¹
            if result and "messages" in result and result["messages"]:
                final_msg = result["messages"][-1]
                if hasattr(final_msg, "content") and final_msg.content:
                    content = final_msg.content
                elif hasattr(final_msg, "text"):
                    content = final_msg.text() if callable(final_msg.text) else final_msg.text
                else:
                    content = str(final_msg)

                return {"response": content, "thread_id": request.thread_id, "status": "completed"}
            else:
                return {"response": "æŠ±æ­‰ï¼Œæš‚æ— å¯è¿”å›å†…å®¹ã€‚", "thread_id": request.thread_id, "status": "completed"}

        except GraphInterrupt as e:
            # æ•è· human_assistance å·¥å…·è§¦å‘çš„ interrupt()
            logger.info(f"[customize_state.chat] GraphInterrupt: {e}")
            data = e.interrupts[0] if e.interrupts else {}
            question = data.get("question", "éœ€è¦äººå·¥å®¡é˜…")
            name = data.get("name", "")
            birthday = data.get("birthday", "")
            return {
                "intervention_required": True,
                "thread_id": request.thread_id,
                "question": question,
                "name": name,
                "birthday": birthday,
                "status": "intervention_required",
            }

    except Exception as exc:
        logger.exception("/customize-state/chat å¤„ç†å¤±è´¥")
        raise HTTPException(status_code=500, detail=f"å¯¹è¯å¤„ç†å¤±è´¥: {exc}")


@router.post("/respond")
async def respond_customize_state(request: HumanReview) -> Dict[str, Any]:
    """
    æäº¤äººå·¥å®¡é˜…ç»“æœï¼Œæ¢å¤å›¾æ‰§è¡Œã€‚
    - è‹¥ correct ä»¥ y/Y å¼€å¤´ï¼Œè¡¨ç¤ºç¡®è®¤æ— è¯¯ï¼Œä»…ä¼ é€’ {"correct": correct}
    - å¦åˆ™å¯ä¼ é€’ä¿®æ­£åçš„ name/birthday
    è¿”å›ï¼š{"response": str, "thread_id": str, "status": "completed"}
    """
    try:
        config = {"configurable": {"thread_id": request.thread_id}}

        resume_payload: Dict[str, Any] = {}
        if request.correct:
            resume_payload["correct"] = request.correct
        if request.name is not None:
            resume_payload["name"] = request.name
        if request.birthday is not None:
            resume_payload["birthday"] = request.birthday

        # ä¿åº•ï¼šè‹¥ç”¨æˆ·æ—¢æœªæä¾› correctï¼Œä¹Ÿæœªæä¾› name/birthdayï¼Œåˆ™ä»æä¾›ç©ºç»“æ„ï¼Œé¿å… KeyError
        human_command = Command(resume=resume_payload or {"correct": "y"})
        logger.info(f"[customize_state.respond] thread_id={request.thread_id} resume={human_command}")

        final_msg = None
        async for event in graph.astream(human_command, config, stream_mode="values"):
            if isinstance(event, dict) and "messages" in event and event["messages"]:
                final_msg = event["messages"][-1]

        if final_msg is not None:
            if hasattr(final_msg, "content") and final_msg.content:
                content = final_msg.content
            elif hasattr(final_msg, "text"):
                content = final_msg.text() if callable(final_msg.text) else final_msg.text
            else:
                content = str(final_msg)

            return {"response": content, "thread_id": request.thread_id, "status": "completed"}

        return {"response": "äººå·¥å®¡é˜…å·²å¤„ç†ã€‚", "thread_id": request.thread_id, "status": "completed"}

    except Exception as exc:
        logger.exception("/customize-state/respond å¤„ç†å¤±è´¥")
        raise HTTPException(status_code=500, detail=f"äººå·¥å®¡é˜…å¤„ç†å¤±è´¥: {exc}")


# ========== SSE æµå¼ç«¯ç‚¹ ==========

@router.get("/chat/stream")
async def stream_chat_customize_state(message: str, thread_id: str = "default"):
    """
    æµå¼å¯¹è¯ï¼ˆSSEï¼‰ã€‚
    äº‹ä»¶ç±»å‹ï¼šstart/content/ai_decision/tool_call/tool_result/intervention_required/end/error
    ä¸ human_loop_routes.py å¯¹é½ï¼Œä¾¿äºå‰ç«¯å¤ç”¨ã€‚
    """
    async def generate():
        logger.info(f"[customize_state.stream_chat] å¼€å§‹è¯·æ±‚: thread_id={thread_id}, message={message[:80]}...")
        try:
            # start äº‹ä»¶
            start_data = {
                'type': 'start',
                'content': '',
                'metadata': {'node': 'system', 'step': 0}
            }
            yield f"data: {json.dumps(start_data)}\n\n"

            # åˆå§‹åŒ–
            config = {"configurable": {"thread_id": thread_id}}
            initial_state = {"messages": [{"role": "user", "content": message}]}
            accumulated_content = ""
            chunk_count = 0
            current_node = None
            tool_decision_sent = False

            logger.info(f"[customize_state.stream_chat] è°ƒç”¨ graph.astreamï¼Œinitial_state={initial_state}")

            try:
                async for stream_mode, chunk in graph.astream(
                    initial_state,
                    config,
                    stream_mode=["messages", "updates"],
                ):
                    if stream_mode == "messages":
                        message_chunk, metadata = chunk
                        node_from_metadata = metadata.get('langgraph_node', 'unknown')
                        # è·³è¿‡ tools èŠ‚ç‚¹çš„æ¶ˆæ¯åˆ†ç‰‡ï¼Œé¿å…é‡å¤/æ— ç”¨å†…å®¹
                        if node_from_metadata == "tools":
                            logger.info(f"[customize_state.stream_chat] è·³è¿‡toolsæ¶ˆæ¯åˆ†ç‰‡: {getattr(message_chunk, 'content', '')}")
                            continue
                        if hasattr(message_chunk, 'content') and message_chunk.content:
                            logger.info(f"[customize_state.stream_chat] æ”¶åˆ°LLMå†…å®¹åˆ†ç‰‡: {message_chunk.content}")
                            chunk_count += 1
                            accumulated_content += message_chunk.content
                            chunk_data = {
                                'type': 'content',
                                'content': message_chunk.content,
                                'metadata': {
                                    'node': node_from_metadata,
                                    'chunk_number': chunk_count,
                                    'accumulated_length': len(accumulated_content),
                                    'langgraph_metadata': metadata,
                                }
                            }
                            yield f"data: {json.dumps(chunk_data)}\n\n"

                        # æ— è®ºæ˜¯å¦æœ‰å¯è§contentï¼Œéƒ½æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨è®¡åˆ’
                        try:
                            tool_calls = getattr(message_chunk, 'tool_calls', None)
                        except Exception:
                            tool_calls = None

                        if node_from_metadata == "chatbot" and tool_calls:
                            # è®°å½•å¹¶å®£å¸ƒAIçš„å·¥å…·è°ƒç”¨å†³ç­–
                            logger.info(f"[customize_state.stream_chat] ğŸ¤– æ£€æµ‹åˆ°AIå†³å®šè°ƒç”¨å·¥å…·: {tool_calls}")
                            ai_decision_evt = {
                                'type': 'ai_decision',
                                'content': 'AIå†³å®šè°ƒç”¨å·¥å…·',
                                'metadata': {
                                    'node': node_from_metadata,
                                    'langgraph_metadata': metadata,
                                }
                            }
                            yield f"data: {json.dumps(ai_decision_evt)}\n\n"

                            # é€ä¸ªå‘é€å…·ä½“å·¥å…·è°ƒç”¨äº‹ä»¶
                            for tc in tool_calls:
                                # å…¼å®¹dictä¸å¯¹è±¡ä¸¤ç§ç»“æ„
                                tool_name = None
                                tool_args = {}
                                if isinstance(tc, dict):
                                    tool_name = tc.get('name') or tc.get('tool')
                                    tool_args = tc.get('args') or {}
                                else:
                                    tool_name = getattr(tc, 'name', None) or getattr(tc, 'tool', None)
                                    tool_args = getattr(tc, 'args', {}) or {}

                                logger.info(f"[customize_state.stream_chat] ğŸ› ï¸ å·¥å…·è°ƒç”¨è®¡åˆ’ -> tool={tool_name}, args={tool_args}")
                                tool_call_evt = {
                                    'type': 'tool_call',
                                    'tool': tool_name,
                                    'args': tool_args,
                                    'metadata': {
                                        'node': node_from_metadata,
                                        'langgraph_metadata': metadata,
                                    }
                                }
                                yield f"data: {json.dumps(tool_call_evt)}\n\n"

                    elif stream_mode == "updates":
                        for node_name, node_output in chunk.items():
                            current_node = node_name
                            logger.info(f"[customize_state.stream_chat] èŠ‚ç‚¹æ›´æ–° - {node_name}: {str(node_output)}")

                            # å¤„ç†ä¸­æ–­èŠ‚ç‚¹ï¼ˆäººå·¥å®¡é˜…ï¼‰
                            if node_name in {"__interrupt__", "interrupt", "graph:interrupt"}:
                                interrupt_payload = None
                                try:
                                    candidate = node_output[0] if isinstance(node_output, (list, tuple)) and node_output else node_output
                                    if hasattr(candidate, "value"):
                                        interrupt_payload = candidate.value
                                    elif isinstance(candidate, dict) and "value" in candidate:
                                        interrupt_payload = candidate.get("value")
                                except Exception:
                                    interrupt_payload = None

                                question = None
                                name = None
                                birthday = None
                                if isinstance(interrupt_payload, dict):
                                    question = interrupt_payload.get("question")
                                    name = interrupt_payload.get("name")
                                    birthday = interrupt_payload.get("birthday")
                                if not question:
                                    question = "éœ€è¦äººå·¥å®¡é˜…"

                                logger.info(f"[customize_state.stream_chat] æ£€æµ‹åˆ°ä¸­æ–­ï¼Œquestion={question} name={name} birthday={birthday}")
                                evt = {
                                    'type': 'intervention_required',
                                    'question': question,
                                    'name': name,
                                    'birthday': birthday,
                                    'thread_id': thread_id,
                                    'metadata': {'node': node_name}
                                }
                                yield f"data: {json.dumps(evt, ensure_ascii=False)}\n\n"

                                end_data = {
                                    'type': 'end',
                                    'content': '',
                                    'metadata': {
                                        'total_chunks': chunk_count,
                                        'total_length': len(accumulated_content),
                                        'final_node': current_node,
                                    }
                                }
                                yield f"data: {json.dumps(end_data)}\n\n"
                                return

                            if node_name == "tools":
                                logger.info("[customize_state.stream_chat] ğŸ”§ å·¥å…·èŠ‚ç‚¹æ‰§è¡Œä¸­...")
                                # å…ˆå‘é€å·¥å…·è¿è¡Œä¸­äº‹ä»¶ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·åé¦ˆ
                                running_evt = {
                                    'type': 'tool_running',
                                    'content': 'ğŸ”§ æ­£åœ¨æ‰§è¡Œå·¥å…·...',
                                    'metadata': {'node': node_name}
                                }
                                yield f"data: {json.dumps(running_evt)}\n\n"
                                if "messages" in node_output and node_output["messages"]:
                                    tool_message = node_output["messages"][-1]
                                    if hasattr(tool_message, 'content'):
                                        logger.info(f"[customize_state.stream_chat] âœ… å·¥å…·æ‰§è¡Œå®Œæˆ")
                                        tool_data = {
                                            'type': 'tool_result',
                                            'content': 'ğŸ”§ å·¥å…·æ‰§è¡Œå®Œæˆ',
                                            'result': tool_message.content,
                                            'metadata': {'node': node_name}
                                        }
                                        yield f"data: {json.dumps(tool_data)}\n\n"

                            elif node_name == "chatbot":
                                if "messages" in node_output and node_output["messages"]:
                                    ai_message = node_output["messages"][-1]
                                    if hasattr(ai_message, 'tool_calls') and ai_message.tool_calls and not tool_decision_sent:
                                        tool_decision_sent = True
                                        decision_data = {
                                            'type': 'ai_decision',
                                            'content': 'ğŸ¤– AIå†³å®šè°ƒç”¨å·¥å…·',
                                            'metadata': {'node': node_name}
                                        }
                                        logger.info(f"[customize_state.stream_chat] ğŸ“¤ å‘é€AIå†³ç­–äº‹ä»¶: {decision_data}")
                                        yield f"data: {json.dumps(decision_data)}\n\n"
                                        await asyncio.sleep(0.01)
                                        for tool_call in ai_message.tool_calls:
                                            tool_call_data = {
                                                'type': 'tool_call',
                                                'content': f"ğŸ” å‡†å¤‡è°ƒç”¨å·¥å…·: {tool_call.get('name','unknown')}",
                                                'tool_name': tool_call.get('name','unknown'),
                                                'tool_args': tool_call.get('args', {}),
                                                'metadata': {'node': node_name}
                                            }
                                            logger.info(f"[customize_state.stream_chat] ğŸ“¤ å‘é€å·¥å…·è°ƒç”¨äº‹ä»¶: {tool_call_data}")
                                            yield f"data: {json.dumps(tool_call_data)}\n\n"
                                            await asyncio.sleep(0.01)

                end_data = {
                    'type': 'end',
                    'content': '',
                    'metadata': {
                        'total_chunks': chunk_count,
                        'total_length': len(accumulated_content),
                        'final_node': current_node,
                    }
                }
                logger.info(f"[customize_state.stream_chat] å‘é€ç»“æŸäº‹ä»¶: {end_data}")
                yield f"data: {json.dumps(end_data)}\n\n"

            except GraphInterrupt as e:
                data = e.interrupts[0] if e.interrupts else {}
                question = data.get("question", "éœ€è¦äººå·¥å®¡é˜…")
                name = data.get("name")
                birthday = data.get("birthday")
                logger.info(f"[customize_state.stream_chat] æ•è·GraphInterrupt: question={question}")
                yield f"data: {json.dumps({'type':'intervention_required','question':question,'name':name,'birthday':birthday,'thread_id':thread_id}, ensure_ascii=False)}\n\n"

        except Exception as e:
            logger.error(f"[customize_state.stream_chat] æµå¼å¯¹è¯å¤±è´¥: {str(e)}", exc_info=True)
            yield f"data: {json.dumps({'type':'error','error':str(e)}, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        },
    )


@router.get("/respond/stream")
async def stream_respond_customize_state(
    thread_id: str,
    response: Optional[str] = None,
    correct: Optional[str] = None,
    name: Optional[str] = None,
    birthday: Optional[str] = None,
):
    """
    äººå·¥å›å¤åæµå¼æ¢å¤ï¼ˆSSEï¼‰ã€‚äº‹ä»¶ç»“æ„ä¸ /chat/stream å¯¹é½ã€‚
    """
    async def generate():
        logger.info(f"[customize_state.respond_stream] å¼€å§‹æµå¼æ¢å¤: thread_id={thread_id}, response={(response[:80] if response else '')}...")
        try:
            # start äº‹ä»¶
            start_data = {'type': 'start', 'content': '', 'metadata': {'node': 'system', 'step': 0}}
            yield f"data: {json.dumps(start_data)}\n\n"

            config = {"configurable": {"thread_id": thread_id}}
            # æ„é€  resume è´Ÿè½½ï¼šä¼˜å…ˆ correctï¼Œå…¶æ¬¡ name/birthdayï¼›ä¿ç•™ response ä½œä¸ºå¤‡æ³¨ï¼ˆä¸å‚ä¸ resumeï¼‰
            resume_payload: Dict[str, Any] = {}
            # ä½¿ç”¨å‚æ•°åˆ«åï¼Œé¿å…åç»­åœ¨é—­åŒ…ä¸­å¯¹ name/birthday çš„èµ‹å€¼é€ æˆé®è”½
            req_name = name
            req_birthday = birthday
            if correct:
                resume_payload["correct"] = correct
            if req_name is not None:
                resume_payload["name"] = req_name
            if req_birthday is not None:
                resume_payload["birthday"] = req_birthday
            human_command = Command(resume=resume_payload or {"correct": "y"})
            logger.info(
                f"[customize_state.respond_stream] thread_id={thread_id} resume={resume_payload or {'correct':'y'}}, note={response[:80] if response else ''}"
            )

            accumulated_content = ""
            chunk_count = 0
            current_node = None
            tool_decision_sent = False

            async for stream_mode, chunk in graph.astream(
                human_command, config, stream_mode=["messages", "updates"]
            ):
                if stream_mode == "messages":
                    message_chunk, metadata = chunk
                    node_from_metadata = metadata.get('langgraph_node', 'unknown')
                    if node_from_metadata == "tools":
                        logger.info(f"[customize_state.respond_stream] è·³è¿‡toolsæ¶ˆæ¯åˆ†ç‰‡: {getattr(message_chunk, 'content', '')}")
                        continue
                    if hasattr(message_chunk, 'content') and message_chunk.content:
                        logger.info(f"[customize_state.respond_stream] æ”¶åˆ°LLMå†…å®¹åˆ†ç‰‡: {message_chunk.content}")
                        chunk_count += 1
                        accumulated_content += message_chunk.content
                        yield f"data: {json.dumps({'type':'content','content':message_chunk.content,'metadata':{'node':node_from_metadata,'chunk_number':chunk_count,'accumulated_length':len(accumulated_content),'langgraph_metadata':metadata}})}\n\n"

                elif stream_mode == "updates":
                    for node_name, node_output in chunk.items():
                        current_node = node_name
                        logger.info(f"[customize_state.respond_stream] èŠ‚ç‚¹æ›´æ–° - {node_name}: {str(node_output)}")

                        if node_name in {"__interrupt__", "interrupt", "graph:interrupt"}:
                            interrupt_payload = None
                            try:
                                candidate = node_output[0] if isinstance(node_output, (list, tuple)) and node_output else node_output
                                if hasattr(candidate, "value"):
                                    interrupt_payload = candidate.value
                                elif isinstance(candidate, dict) and "value" in candidate:
                                    interrupt_payload = candidate.get("value")
                            except Exception:
                                interrupt_payload = None
                            question = interrupt_payload.get("question") if isinstance(interrupt_payload, dict) else None
                            intr_name = interrupt_payload.get("name") if isinstance(interrupt_payload, dict) else None
                            intr_birthday = interrupt_payload.get("birthday") if isinstance(interrupt_payload, dict) else None
                            if not question:
                                question = "éœ€è¦äººå·¥ååŠ©"
                            yield f"data: {json.dumps({'type':'intervention_required','question':question,'name':intr_name,'birthday':intr_birthday,'thread_id':thread_id,'metadata':{'node':node_name}}, ensure_ascii=False)}\n\n"
                            end_data = {'type':'end','content':'','metadata':{'total_chunks':chunk_count,'total_length':len(accumulated_content),'final_node':current_node}}
                            yield f"data: {json.dumps(end_data)}\n\n"
                            return

                        if node_name == "tools":
                            # å…ˆå‘ŠçŸ¥å·¥å…·æ­£åœ¨æ‰§è¡Œï¼ˆé€šç”¨æ–‡æ¡ˆï¼Œé¿å…ä¸å…·ä½“è”ç½‘æœç´¢ç»‘å®šï¼‰
                            running_evt = {'type':'tool_running','content':'ğŸ”§ æ­£åœ¨æ‰§è¡Œå·¥å…·...','metadata':{'node':node_name}}
                            yield f"data: {json.dumps(running_evt)}\n\n"
                            if "messages" in node_output and node_output["messages"]:
                                tool_message = node_output["messages"][-1]
                                if hasattr(tool_message, 'content'):
                                    tool_data = {'type':'tool_result','content':'ğŸ”§ å·¥å…·æ‰§è¡Œå®Œæˆ','result':tool_message.content,'metadata':{'node':node_name}}
                                    yield f"data: {json.dumps(tool_data)}\n\n"

                        elif node_name == "chatbot":
                            if "messages" in node_output and node_output["messages"]:
                                ai_message = node_output["messages"][-1]
                                if hasattr(ai_message, 'tool_calls') and ai_message.tool_calls and not tool_decision_sent:
                                    tool_decision_sent = True
                                    decision_data = {'type':'ai_decision','content':'ğŸ¤– AIå†³å®šè°ƒç”¨å·¥å…·','metadata':{'node':node_name}}
                                    yield f"data: {json.dumps(decision_data)}\n\n"
                                    await asyncio.sleep(0.01)
                                    for tool_call in ai_message.tool_calls:
                                        tool_call_data = {
                                            'type': 'tool_call',
                                            'content': f"ğŸ” å‡†å¤‡è°ƒç”¨å·¥å…·: {tool_call.get('name','unknown')}",
                                            'tool_name': tool_call.get('name','unknown'),
                                            'tool_args': tool_call.get('args', {}),
                                            'metadata': {'node': node_name}
                                        }
                                        yield f"data: {json.dumps(tool_call_data)}\n\n"
                                        await asyncio.sleep(0.01)

            end_data = {'type':'end','content':'','metadata':{'total_chunks':chunk_count,'total_length':len(accumulated_content),'final_node':current_node}}
            logger.info(f"[customize_state.respond_stream] æµå¼æ¢å¤å®Œæˆ: {str(end_data)}")
            yield f"data: {json.dumps(end_data)}\n\n"

        except Exception as e:
            logger.error(f"[customize_state.respond_stream] æµå¼æ¢å¤å¤±è´¥: {str(e)}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)}, ensure_ascii=False)}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream",
        },
    )

